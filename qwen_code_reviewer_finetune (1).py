# -*- coding: utf-8 -*-
"""qwen_code_reviewer_finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DRftZbBz201QBpchRxNfFjmRWNFj5xOU
"""

pip install -q transformers datasets bitsandbytes accelerate trl

!ls

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from transformers import DataCollatorForLanguageModeling

model_name = "Qwen/Qwen2.5-Coder-7B"

tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,

    device_map="auto",
    load_in_4bit=True,
    trust_remote_code=True,
)
model.config.use_cache=False

from peft import LoraConfig, get_peft_model, TaskType

lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj"
    ]
)

model = get_peft_model(model, lora_config)

model.print_trainable_parameters()

with open("code_reviewer_train.jsonl", "r") as f:
    print(f.readline())

import json

with open("code_reviewer_train.jsonl") as f:
    for i, line in enumerate(f):
        try:
            json.loads(line)
        except Exception as e:
            print(f"‚ùå Error at line {i}: {e}")
            break
    else:
        print("‚úÖ All lines are valid JSON")

from datasets import load_dataset

dataset = load_dataset(
    "json",
    data_files="code_reviewer_train.jsonl",
    split="train"
)

print(dataset)

def tokenize_fn(example):
    text = (
        f"### Instruction:\n{example['instruction']}\n\n"
        f"### Code:\n{example['input']}\n\n"
        f"### Review:\n{example['output']}"
    )

    tokenized = tokenizer(
        text,
        truncation=True,
        padding="max_length",
        max_length=512,
        return_tensors=None,
    )

    tokenized["labels"] = tokenized["input_ids"].copy()

    return tokenized

tokenized_dataset = dataset.map(
    tokenize_fn,
    remove_columns=dataset.column_names,
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

ex = tokenized_dataset[0]

print(type(ex["input_ids"]), type(ex["labels"]))
print(len(ex["input_ids"]), len(ex["labels"]))
print(isinstance(ex["labels"][0], int))

print(tokenized_dataset[0].keys())

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./qwen-code-reviewer",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=1,
    save_strategy="epoch",
    optim="paged_adamw_8bit",
    report_to="none",
    remove_unused_columns=False  # üî¥ THIS IS THE FIX
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
)

trainer.train()

trainer.save_model("qwen_code_reviewer_lora")
tokenizer.save_pretrained("qwen_code_reviewer_lora")

from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-Coder-7B",
    device_map="auto",
    load_in_4bit=True,
)

model = PeftModel.from_pretrained(
    base_model,
    "qwen_code_reviewer_lora"
)

model.eval()

prompt = """### Instruction:
Review the following code and identify issues and improvements.

### Code:
def login(user, pwd):
    if user == "admin" and pwd == "1234":
        return True
    return False

### Review:
"""

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

output = model.generate(
    **inputs,
    max_new_tokens=200,
    temperature=0.3,
)

print(tokenizer.decode(output[0], skip_special_tokens=True))

### ‚ÄúFine-tuned Qwen2.5-Coder-7B using QLoRA for automated code review with security-focused reasoning.‚Äù

